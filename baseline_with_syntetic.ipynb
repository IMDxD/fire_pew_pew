{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f171bfc3-5047-4c18-92f0-e6822f28dfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import lightgbm as lgb\n",
    "from sklearn import metrics\n",
    "from catboost import CatBoost, CatBoostClassifier, Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6305fbc6-1c62-446f-bb12-5541946da97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/roelbertens/time-series-nested-cv/blob/master/time_series_cross_validation/custom_time_series_split.py\n",
    "class CustomTimeSeriesSplit:\n",
    "\n",
    "    def __init__(self,\n",
    "                 train_set_size: int,\n",
    "                 test_set_size: int\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        :param train_set_size: data points (days) in each fold for the train set\n",
    "        :param test_set_size: data points (days) in each fold for the test set\n",
    "        \"\"\"\n",
    "        self.train_set_size = train_set_size\n",
    "        self.test_set_size = test_set_size\n",
    "        self._logger = logging.getLogger(__name__)\n",
    "\n",
    "    def split(self,\n",
    "              x: np.array,\n",
    "              y: np.array = None) -> (np.array, np.array):\n",
    "        \"\"\"Return train/test split indices.\n",
    "        :param x: time series to use for prediction, shape (n_samples, n_features)\n",
    "        :param y: time series to predict, shape (n_samples, n_features)\n",
    "        :return: (train_indices, test_indices)\n",
    "        Note: index of both x and y should be of type datetime.\n",
    "        \"\"\"\n",
    "        if y is not None:\n",
    "            assert x.index.equals(y.index)\n",
    "        split_points = self.get_split_points(x)\n",
    "        for split_point in split_points:\n",
    "            is_train = (x.index < split_point) & (x.index >= split_point -\n",
    "                                                  pd.Timedelta(self.train_set_size, unit='D'))\n",
    "            is_test = (x.index >= split_point) & (x.index < split_point +\n",
    "                                                  pd.Timedelta(self.test_set_size, unit='D'))\n",
    "            if not is_train.any() or not is_test.any():\n",
    "                self._logger.warning('Found %d train and %d test observations '\n",
    "                                     'skipping fold for split point %s',\n",
    "                                     is_train.sum(), is_test.sum(), split_point)\n",
    "                continue\n",
    "            dummy_ix = pd.Series(range(0, len(x)), index=x.index)\n",
    "            ix_train = dummy_ix.loc[is_train].values\n",
    "            ix_test = dummy_ix.loc[is_test].values\n",
    "            if ix_train is None or ix_test is None:\n",
    "                self._logger.warning('Found no data for train or test period, '\n",
    "                                     'skipping fold for split date %s',\n",
    "                                     split_point)\n",
    "                continue\n",
    "            yield ix_train, ix_test\n",
    "\n",
    "    def get_split_points(self, x: np.array) -> pd.DatetimeIndex:\n",
    "        \"\"\"Get all possible split point dates\"\"\"\n",
    "        start = x.index.min() + pd.Timedelta(self.train_set_size, unit='D')\n",
    "        end = x.index.max() - pd.Timedelta(self.test_set_size - 1, unit='D')\n",
    "        self._logger.info(f'Generating split points from {start} to {end}')\n",
    "        split_range = pd.date_range(start, end, freq='D')\n",
    "        first_split_point =  (len(split_range) + self.test_set_size - 1) % self.test_set_size\n",
    "        return split_range[first_split_point::self.test_set_size]\n",
    "    \n",
    "    \n",
    "class ModelBuilder:\n",
    "    def __init__(self, df, target, feats, cat_feats):\n",
    "        self.df = df\n",
    "        self.target = target\n",
    "        self.feats = feats\n",
    "        self.cat_feats = cat_feats\n",
    "        self.mode = 'classification' if type(target)==str else 'multiclassification'\n",
    "            \n",
    "    def train_folds(self, train_size=120, test_size=30, iterations=1000, early_stopping=False):\n",
    "        if self.mode == 'classification':\n",
    "            oof_preds = np.zeros(self.df.shape[0])\n",
    "        else:\n",
    "            oof_preds = np.zeros((self.df.shape[0], len(targets)))\n",
    "            \n",
    "        folds_mask = np.zeros(oof_preds.shape[0])\n",
    "        for fold_, (train_index, test_index) in enumerate(CustomTimeSeriesSplit(train_set_size=train_size, test_set_size=test_size).split(self.df)):\n",
    "            X_train, y_train = self.df.iloc[train_index,:][self.feats], self.df.iloc[train_index,:][self.target]\n",
    "            X_val, y_val = self.df.iloc[test_index,:][self.feats], self.df.iloc[test_index,:][self.target]\n",
    "\n",
    "            weeks_train = X_train.reset_index()['dt']\n",
    "            weeks_test = X_val.reset_index()['dt']\n",
    "\n",
    "            tr_start_week = weeks_train.min()\n",
    "            tr_end_week = weeks_train.max()\n",
    "            ts_start_week = weeks_test.min()\n",
    "            ts_end_week = weeks_test.max()\n",
    "            \n",
    "            print()\n",
    "            print()\n",
    "            print(f'Fold {fold_} train ({tr_start_week}, {tr_end_week}) test ({ts_start_week}, {ts_end_week})')\n",
    "            \n",
    "            \n",
    "            cat_model = CatBoostClassifier(\n",
    "                iterations=iterations,\n",
    "                learning_rate=0.05,\n",
    "                metric_period=500,\n",
    "                loss_function='Logloss' if self.mode=='classification' else 'MultiLogloss',\n",
    "                l2_leaf_reg=10,\n",
    "                eval_metric='F1' if self.mode=='classification' else 'MultiLogloss', \n",
    "                task_type='CPU',\n",
    "                early_stopping_rounds=100,\n",
    "                random_seed=1234,\n",
    "                use_best_model=early_stopping\n",
    "                )\n",
    "            \n",
    "            D_train = Pool(X_train, y_train, cat_features=cat_feats, feature_names=feats)\n",
    "            D_val = Pool(X_val, y_val, cat_features=cat_feats, feature_names=feats)\n",
    "            \n",
    "            print('Train catboost')\n",
    "            cat_model.fit(\n",
    "                D_train, \n",
    "                eval_set=D_val if early_stopping else None,\n",
    "                verbose=True,\n",
    "                plot=False\n",
    "            )\n",
    "            \n",
    "            if self.mode == 'classification':\n",
    "                D_train_lgb = lgb.Dataset(X_train, y_train, weight=None, free_raw_data=False)\n",
    "                D_val_lgb = lgb.Dataset(X_val, y_val, weight=None, free_raw_data=False)\n",
    "                \n",
    "                print('Train lgbm')\n",
    "                lgbm_model = lgb.train(\n",
    "                    {\n",
    "                    'objective': 'binary',\n",
    "                    'feature_pre_filter': False,\n",
    "                    'lambda_l1': 5.246525412521277e-08,\n",
    "                    'lambda_l2': 3.963188589061798e-05,\n",
    "                    'num_leaves': 6,\n",
    "                    'feature_fraction': 0.7,\n",
    "                    'bagging_fraction': 1.0,\n",
    "                    'bagging_freq': 0,\n",
    "                    'min_child_samples': 20,\n",
    "                    },\n",
    "                   D_train_lgb,\n",
    "                   num_boost_round=iterations,\n",
    "                   early_stopping_rounds=200 if early_stopping else None,\n",
    "                   valid_sets=D_val_lgb if early_stopping else None,\n",
    "                   feature_name=feats,\n",
    "                   verbose_eval=500\n",
    "                  )\n",
    "                preds = (0.5*cat_model.predict_proba(X_val)[:,1] + 0.5*lgbm_model.predict(X_val))\n",
    "                print()\n",
    "                print(f'Fold {fold_} F1 Score ', metrics.f1_score(y_val, preds.round()))\n",
    "                print(f'Fold {fold_} ROC AUC Score ', metrics.roc_auc_score(y_val, preds.round()))\n",
    "                print(f'Fold {fold_} Confusion matrix')\n",
    "                print(metrics.confusion_matrix(y_val, preds.round()))\n",
    "                oof_preds[test_index] = preds\n",
    "            else:\n",
    "                oof_preds[test_index] = cat_model.predict(X_val)\n",
    "                print(f'Fold {fold_} F1 Score ', metrics.f1_score(y_val, oof_preds[test_index].round(), average='micro'))\n",
    "                try:\n",
    "                    print(f'Fold {fold_} ROC AUC Score ', metrics.roc_auc_score(y_val, oof_preds[test_index]))\n",
    "                except ValueError:\n",
    "                    print(f'Fold {fold_} ROC AUC Score ', 0)\n",
    "                    \n",
    "            folds_mask[test_index] = 1\n",
    "        \n",
    "        if self.mode == 'classification':\n",
    "            oof_f1micro = metrics.f1_score(self.df.iloc[folds_mask==1,:][self.target], oof_preds[folds_mask==1].round(), average='micro')\n",
    "            oof_f1micro = metrics.roc_auc_score(self.df.iloc[folds_mask==1,:][self.target], oof_preds[folds_mask==1], average='micro')\n",
    "        else:\n",
    "            oof_f1micro = metrics.f1_score(self.df.iloc[folds_mask==1,:][self.target], oof_preds[folds_mask==1].round(), average='micro')\n",
    "            oof_f1micro = metrics.roc_auc_score(self.df.iloc[folds_mask==1,:][self.target], oof_preds[folds_mask==1], average='micro')\n",
    "        \n",
    "        print()\n",
    "        print('Overall OOF F1 Micro ', oof_f1micro)\n",
    "        print('Overall OOF Mean ROC AUC Score ', oof_f1micro)\n",
    "        \n",
    "    def train_final_models(self, iterations=1000, early_stopping=False):\n",
    "        if self.mode == 'classification':\n",
    "            X_train, y_train = self.df.iloc[:,:][self.feats], self.df.iloc[:,:][self.target]\n",
    "            \n",
    "      \n",
    "            \n",
    "            cat_model = CatBoostClassifier(\n",
    "                iterations=iterations,\n",
    "                learning_rate=0.05,\n",
    "                metric_period=500,\n",
    "                loss_function='Logloss',\n",
    "                l2_leaf_reg=10,\n",
    "                eval_metric='F1', \n",
    "                task_type='CPU',\n",
    "                random_seed=1234,\n",
    "                use_best_model=early_stopping\n",
    "                )\n",
    "            \n",
    "            D_train = Pool(X_train, y_train, cat_features=cat_feats, feature_names=feats)\n",
    "            \n",
    "            print('Train catboost')\n",
    "            cat_model.fit(\n",
    "                D_train, \n",
    "                eval_set=None,\n",
    "                verbose=True,\n",
    "                plot=False\n",
    "            )\n",
    "            \n",
    "            D_train_lgb = lgb.Dataset(X_train, y_train, weight=None, free_raw_data=False)\n",
    "\n",
    "            print('Train lgbm')\n",
    "            lgbm_model = lgb.train(\n",
    "                {\n",
    "                'objective': 'binary',\n",
    "                'feature_pre_filter': False,\n",
    "                'lambda_l1': 5.246525412521277e-08,\n",
    "                'lambda_l2': 3.963188589061798e-05,\n",
    "                'num_leaves': 6,\n",
    "                'feature_fraction': 0.7,\n",
    "                'bagging_fraction': 1.0,\n",
    "                'bagging_freq': 0,\n",
    "                'min_child_samples': 20,\n",
    "                },\n",
    "               D_train_lgb,\n",
    "               num_boost_round=iterations,\n",
    "               valid_sets=None,\n",
    "               feature_name=feats,\n",
    "               verbose_eval=500\n",
    "              )\n",
    "            \n",
    "            return cat_model, lgbm_model\n",
    "            \n",
    "        elif self.mode == 'multiclassification':\n",
    "            raise NotImplementedError "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0cf34c8-cb1a-4da4-b3a8-f6068764ae37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a44b8dd-ecfc-4eac-969f-f4261f9ae3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CONFIDENCE'] = df['CONFIDENCE'].map({'l':0, 'h':1, 'n':3})\n",
    "df['SATELLITE'] = df['SATELLITE'].map({'1':0, 'N':1})\n",
    "df['DAYNIGHT'] = df['DAYNIGHT'].map({'D':0, 'N':1})\n",
    "df['dt'] = pd.to_datetime(df['dt']).dt.date\n",
    "df = df.set_index('dt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f17e8e44-600b-4bba-86ce-d1bc33ce7a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['infire_day_1','infire_day_2','infire_day_3','infire_day_4','infire_day_5','infire_day_6','infire_day_7','infire_day_8']\n",
    "feats = ['grid_index','LATITUDE','LONGITUDE','BRIGHTNESS','SCAN','TRACK','ACQ_TIME','SATELLITE','DAYNIGHT','CONFIDENCE','BRIGHT_T31','FRP','TYPE']\n",
    "#cat_feats = ['grid_index', 'DAYNIGHT','SATELLITE']\n",
    "cat_feats = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63dbbe9e-2c7a-4807-a527-58496c20fd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['infire_day_1','infire_day_2','infire_day_3','infire_day_4','infire_day_5','infire_day_6','infire_day_7','infire_day_8']\n",
    "df['target'] = (df[targets].sum(axis=1)>0).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3262f9ab-ea43-4586-860e-a63fc1fc9abd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.842362\n",
       "0    0.157638\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4041550-5666-488b-896e-4b507915a7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### syntetic data\n",
    "DROPOUT_PROBA = 0.7\n",
    "UPSAMPLE_RATE = 6\n",
    "\n",
    "df_syn_base = df[df['target']==0][feats]\n",
    "df_syn_final = pd.DataFrame()\n",
    "\n",
    "for i in range(UPSAMPLE_RATE):\n",
    "    df_syn = df_syn_base.copy()\n",
    "    for f in feats[3:]:\n",
    "        df_syn[f] = df_syn[f].apply(lambda x: x if np.random.random()>DROPOUT_PROBA else None).sample(frac=1.0).values\n",
    "    df_syn_final = pd.concat([df_syn_final, df_syn], axis=0)\n",
    "\n",
    "df_syn_final['target'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acbf840d-14fd-41c7-9427-293e74b44ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.concat([\n",
    "    df[feats+['target']],\n",
    "    df_syn_final], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82a19926-c981-4665-8a5c-1dc87a8d6e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.567092\n",
       "1    0.432908\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined['target'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7bb3c8-1762-4498-982e-d8c330fc80b9",
   "metadata": {},
   "source": [
    "### Single lable (will we see fire during a period of 8 days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a678e62-74c5-4bb9-bfd7-917c16915c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_model = ModelBuilder(df_combined, 'target', feats, cat_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5ca9e17-61d9-4fc5-a84b-e82553df3b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Fold 0 train (2020-01-26, 2020-05-24) test (2020-05-25, 2020-06-23)\n",
      "Train catboost\n",
      "0:\tlearn: 0.9092012\ttotal: 190ms\tremaining: 3m 10s\n",
      "500:\tlearn: 0.9192056\ttotal: 5.71s\tremaining: 5.69s\n",
      "999:\tlearn: 0.9256910\ttotal: 11.3s\tremaining: 0us\n",
      "Train lgbm\n",
      "[LightGBM] [Info] Number of positive: 14305, number of negative: 18620\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001397 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1858\n",
      "[LightGBM] [Info] Number of data points in the train set: 32925, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.434472 -> initscore=-0.263627\n",
      "[LightGBM] [Info] Start training from score -0.263627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a:\\apps\\python3.8\\lib\\site-packages\\lightgbm\\engine.py:240: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 0 F1 Score  0.7801026450848795\n",
      "Fold 0 ROC AUC Score  0.8035772103887693\n",
      "Fold 0 Confusion matrix\n",
      "[[1338  153]\n",
      " [ 404  988]]\n",
      "\n",
      "\n",
      "Fold 1 train (2020-02-25, 2020-06-23) test (2020-06-24, 2020-07-23)\n",
      "Train catboost\n",
      "0:\tlearn: 0.9135035\ttotal: 23.2ms\tremaining: 23.1s\n",
      "500:\tlearn: 0.9202317\ttotal: 6.07s\tremaining: 6.04s\n",
      "999:\tlearn: 0.9258262\ttotal: 12s\tremaining: 0us\n",
      "Train lgbm\n",
      "[LightGBM] [Info] Number of positive: 15654, number of negative: 19992\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001732 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1867\n",
      "[LightGBM] [Info] Number of data points in the train set: 35646, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.439152 -> initscore=-0.244606\n",
      "[LightGBM] [Info] Start training from score -0.244606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a:\\apps\\python3.8\\lib\\site-packages\\lightgbm\\engine.py:240: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1 F1 Score  0.9053108275010291\n",
      "Fold 1 ROC AUC Score  0.92160137954899\n",
      "Fold 1 Confusion matrix\n",
      "[[7863 1244]\n",
      " [ 136 6597]]\n",
      "\n",
      "\n",
      "Fold 2 train (2020-03-26, 2020-07-23) test (2020-07-24, 2020-08-22)\n",
      "Train catboost\n",
      "0:\tlearn: 0.9086546\ttotal: 41.9ms\tremaining: 41.9s\n",
      "500:\tlearn: 0.9140407\ttotal: 6.37s\tremaining: 6.34s\n",
      "999:\tlearn: 0.9196146\ttotal: 12.7s\tremaining: 0us\n",
      "Train lgbm\n",
      "[LightGBM] [Info] Number of positive: 19192, number of negative: 26124\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001623 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1871\n",
      "[LightGBM] [Info] Number of data points in the train set: 45316, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.423515 -> initscore=-0.308361\n",
      "[LightGBM] [Info] Start training from score -0.308361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a:\\apps\\python3.8\\lib\\site-packages\\lightgbm\\engine.py:240: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 2 F1 Score  0.9297418271920711\n",
      "Fold 2 ROC AUC Score  0.9266535318194321\n",
      "Fold 2 Confusion matrix\n",
      "[[5162  837]\n",
      " [  42 5816]]\n",
      "\n",
      "\n",
      "Fold 3 train (2020-04-25, 2020-08-22) test (2020-08-23, 2020-09-21)\n",
      "Train catboost\n",
      "0:\tlearn: 0.9099302\ttotal: 28.5ms\tremaining: 28.5s\n",
      "500:\tlearn: 0.9184891\ttotal: 5.81s\tremaining: 5.79s\n",
      "999:\tlearn: 0.9266217\ttotal: 11.6s\tremaining: 0us\n",
      "Train lgbm\n",
      "[LightGBM] [Info] Number of positive: 15254, number of negative: 20412\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001382 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1866\n",
      "[LightGBM] [Info] Number of data points in the train set: 35666, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.427690 -> initscore=-0.291281\n",
      "[LightGBM] [Info] Start training from score -0.291281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a:\\apps\\python3.8\\lib\\site-packages\\lightgbm\\engine.py:240: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 3 F1 Score  0.9221148379761228\n",
      "Fold 3 ROC AUC Score  0.9201391593592079\n",
      "Fold 3 Confusion matrix\n",
      "[[3007  444]\n",
      " [ 104 3244]]\n",
      "\n",
      "\n",
      "Fold 4 train (2020-05-25, 2020-09-21) test (2020-09-22, 2020-10-21)\n",
      "Train catboost\n",
      "0:\tlearn: 0.9205340\ttotal: 29.6ms\tremaining: 29.6s\n",
      "500:\tlearn: 0.9281676\ttotal: 6.18s\tremaining: 6.15s\n",
      "999:\tlearn: 0.9337143\ttotal: 12.3s\tremaining: 0us\n",
      "Train lgbm\n",
      "[LightGBM] [Info] Number of positive: 17331, number of negative: 20048\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001560 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1864\n",
      "[LightGBM] [Info] Number of data points in the train set: 37379, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.463656 -> initscore=-0.145633\n",
      "[LightGBM] [Info] Start training from score -0.145633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a:\\apps\\python3.8\\lib\\site-packages\\lightgbm\\engine.py:240: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 4 F1 Score  0.8568281938325992\n",
      "Fold 4 ROC AUC Score  0.9210158342772584\n",
      "Fold 4 Confusion matrix\n",
      "[[4589  731]\n",
      " [  49 2334]]\n",
      "\n",
      "\n",
      "Fold 5 train (2020-06-24, 2020-10-21) test (2020-10-22, 2020-11-20)\n",
      "Train catboost\n",
      "0:\tlearn: 0.9114570\ttotal: 17.9ms\tremaining: 17.8s\n",
      "500:\tlearn: 0.9190994\ttotal: 6.35s\tremaining: 6.32s\n",
      "999:\tlearn: 0.9258276\ttotal: 12.7s\tremaining: 0us\n",
      "Train lgbm\n",
      "[LightGBM] [Info] Number of positive: 18322, number of negative: 23877\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001932 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1862\n",
      "[LightGBM] [Info] Number of data points in the train set: 42199, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.434181 -> initscore=-0.264813\n",
      "[LightGBM] [Info] Start training from score -0.264813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a:\\apps\\python3.8\\lib\\site-packages\\lightgbm\\engine.py:240: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 5 F1 Score  0.8176229508196721\n",
      "Fold 5 ROC AUC Score  0.8852268414621574\n",
      "Fold 5 Confusion matrix\n",
      "[[895 134]\n",
      " [ 44 399]]\n",
      "\n",
      "\n",
      "Fold 6 train (2020-07-24, 2020-11-20) test (2020-11-21, 2020-12-20)\n",
      "Train catboost\n",
      "0:\tlearn: 0.9109824\ttotal: 28.6ms\tremaining: 28.6s\n",
      "500:\tlearn: 0.9220326\ttotal: 5.85s\tremaining: 5.83s\n",
      "999:\tlearn: 0.9337218\ttotal: 11.9s\tremaining: 0us\n",
      "Train lgbm\n",
      "[LightGBM] [Info] Number of positive: 12032, number of negative: 15799\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001145 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1858\n",
      "[LightGBM] [Info] Number of data points in the train set: 27831, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.432324 -> initscore=-0.272377\n",
      "[LightGBM] [Info] Start training from score -0.272377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a:\\apps\\python3.8\\lib\\site-packages\\lightgbm\\engine.py:240: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 6 F1 Score  0.822857142857143\n",
      "Fold 6 ROC AUC Score  0.9034930586654724\n",
      "Fold 6 Confusion matrix\n",
      "[[354  52]\n",
      " [ 10 144]]\n",
      "\n",
      "\n",
      "Fold 7 train (2020-08-23, 2020-12-20) test (2020-12-21, 2021-01-19)\n",
      "Train catboost\n",
      "0:\tlearn: 0.8928851\ttotal: 27.8ms\tremaining: 27.8s\n",
      "500:\tlearn: 0.9117011\ttotal: 4.45s\tremaining: 4.43s\n",
      "999:\tlearn: 0.9322221\ttotal: 9.03s\tremaining: 0us\n",
      "Train lgbm\n",
      "[LightGBM] [Info] Number of positive: 6328, number of negative: 10206\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001000 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1848\n",
      "[LightGBM] [Info] Number of data points in the train set: 16534, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.382727 -> initscore=-0.477992\n",
      "[LightGBM] [Info] Start training from score -0.477992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a:\\apps\\python3.8\\lib\\site-packages\\lightgbm\\engine.py:240: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 7 F1 Score  0.786206896551724\n",
      "Fold 7 ROC AUC Score  0.8601557430240103\n",
      "Fold 7 Confusion matrix\n",
      "[[140  21]\n",
      " [ 10  57]]\n",
      "\n",
      "\n",
      "Fold 8 train (2020-09-22, 2021-01-19) test (2021-01-20, 2021-02-18)\n",
      "Train catboost\n",
      "0:\tlearn: 0.8508634\ttotal: 17.6ms\tremaining: 17.5s\n",
      "500:\tlearn: 0.9096079\ttotal: 2.36s\tremaining: 2.35s\n",
      "999:\tlearn: 0.9456353\ttotal: 4.76s\tremaining: 0us\n",
      "Train lgbm\n",
      "[LightGBM] [Info] Number of positive: 3047, number of negative: 6916\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000691 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1845\n",
      "[LightGBM] [Info] Number of data points in the train set: 9963, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.305832 -> initscore=-0.819680\n",
      "[LightGBM] [Info] Start training from score -0.819680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a:\\apps\\python3.8\\lib\\site-packages\\lightgbm\\engine.py:240: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 8 F1 Score  0.7956989247311828\n",
      "Fold 8 ROC AUC Score  0.912951912951913\n",
      "Fold 8 Confusion matrix\n",
      "[[224  35]\n",
      " [  3  74]]\n",
      "\n",
      "\n",
      "Fold 9 train (2020-10-22, 2021-02-18) test (2021-02-19, 2021-03-20)\n",
      "Train catboost\n",
      "0:\tlearn: 0.8358209\ttotal: 3.39ms\tremaining: 3.39s\n",
      "500:\tlearn: 0.9686275\ttotal: 1.38s\tremaining: 1.37s\n",
      "999:\tlearn: 0.9966375\ttotal: 2.72s\tremaining: 0us\n",
      "Train lgbm\n",
      "[LightGBM] [Info] Number of positive: 741, number of negative: 1855\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000567 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1779\n",
      "[LightGBM] [Info] Number of data points in the train set: 2596, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.285439 -> initscore=-0.917639\n",
      "[LightGBM] [Info] Start training from score -0.917639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a:\\apps\\python3.8\\lib\\site-packages\\lightgbm\\engine.py:240: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 9 F1 Score  0.8262411347517731\n",
      "Fold 9 ROC AUC Score  0.8662516615941134\n",
      "Fold 9 Confusion matrix\n",
      "[[443  54]\n",
      " [ 44 233]]\n",
      "\n",
      "\n",
      "Fold 10 train (2020-11-21, 2021-03-20) test (2021-03-21, 2021-04-19)\n",
      "Train catboost\n",
      "0:\tlearn: 0.8524590\ttotal: 37.9ms\tremaining: 37.9s\n",
      "500:\tlearn: 0.9803922\ttotal: 1.2s\tremaining: 1.19s\n",
      "999:\tlearn: 0.9991312\ttotal: 2.34s\tremaining: 0us\n",
      "Train lgbm\n",
      "[LightGBM] [Info] Number of positive: 575, number of negative: 1323\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000648 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1720\n",
      "[LightGBM] [Info] Number of data points in the train set: 1898, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.302950 -> initscore=-0.833287\n",
      "[LightGBM] [Info] Start training from score -0.833287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a:\\apps\\python3.8\\lib\\site-packages\\lightgbm\\engine.py:240: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 10 F1 Score  0.8071157305616631\n",
      "Fold 10 ROC AUC Score  0.8277269166913754\n",
      "Fold 10 Confusion matrix\n",
      "[[2782  270]\n",
      " [ 695 2019]]\n",
      "\n",
      "Overall OOF F1 Micro  0.9363852537115387\n",
      "Overall OOF Mean ROC AUC Score  0.9363852537115387\n"
     ]
    }
   ],
   "source": [
    "fire_model.train_folds(train_size=120, test_size=30, iterations=1000, early_stopping=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb645a6f-5d16-49d0-a1ab-57b183e1fad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train catboost\n",
      "0:\tlearn: 0.9116969\ttotal: 30.5ms\tremaining: 30.5s\n",
      "500:\tlearn: 0.9157650\ttotal: 7.92s\tremaining: 7.88s\n",
      "999:\tlearn: 0.9179507\ttotal: 15.9s\tremaining: 0us\n",
      "Train lgbm\n",
      "[LightGBM] [Info] Number of positive: 37753, number of negative: 49455\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002868 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1872\n",
      "[LightGBM] [Info] Number of data points in the train set: 87208, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.432908 -> initscore=-0.269998\n",
      "[LightGBM] [Info] Start training from score -0.269998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a:\\apps\\python3.8\\lib\\site-packages\\lightgbm\\engine.py:240: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    }
   ],
   "source": [
    "cat_model, lgbm_model = fire_model.train_final_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5890e0fb-4ed2-4fef-b7c3-eecdbe757160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x1e1a17bd310>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_model.save_model('cat_fire', format=\"cbm\")\n",
    "\n",
    "#save just last\n",
    "lgbm_model.save_model('lgbm_model.txt', \n",
    "                      #num_iteration=bst.best_iteration\n",
    "                     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
